library(dplyr)
library(readr)
options(digits=14)

# read in keys to dataset
# keys can be generated by:
# cut -d$'\t' -f 1-4 $filename
DF <- read_tsv("/home/srivbane/shared/caringbridge/data/clean_journals/clean_journals_hom_names_key.txt", col_names=c("site_id", "user_id", "journal_id", "datetime"))
print(nrow(DF))

# remove erroneous dates and remove CaringBridge's test user:
DF <- filter(DF, datetime > 0.86e+12, datetime < 1.5e+12, site_id != 839318)
print(nrow(DF))

# remove duplicated journal entries
DF <- DF %>% group_by(site_id, user_id, journal_id, datetime) %>% filter(row_number() == 1)

# determine which authors have many N or more journal posts
# don't include power users
sites <- DF %>% group_by(site_id) %>% summarise(n_posts = n(), min_date = min(datetime), max_date = max(datetime))
cutoff <- 25
freq_sites <- filter(sites, n_posts >= cutoff,  n_posts < 1000)

freq_sites$first_date = as.POSIXct(freq_sites$min_date/1000, tz="GMT", origin="1970-01-01")
freq_sites$last_date = as.POSIXct(freq_sites$max_date/1000, tz="GMT", origin="1970-01-01")
freq_sites$days_active = as.numeric(difftime(freq_sites$last_date, freq_sites$first_date, units="days"))
freq_sites$days_per_post = freq_sites$days_active / freq_sites$n_posts
print(summary(freq_sites))

keep_sites <- freq_sites %>% select(site_id, n_posts) %>% arrange(site_id)
output <- as.data.frame(keep_sites)
write_tsv(output, "/home/srivbane/shared/caringbridge/data/topic_model_hom_names/keep_sites.txt", col_names=FALSE)
