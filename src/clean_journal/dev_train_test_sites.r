library(dplyr)
library(readr)
options(digits=14)

# read in keys to dataset
# keys can be generated by:
# cut -d$'\t' -f 1-4 $filename
DF <- read_tsv("/home/srivbane/shared/caringbridge/data/dev/clean_journals/clean_journals_no_names_key.txt", col_names=c("site_id", "user_id", "journal_id", "datetime"))

# remove erroneous dates
# and remove CaringBridge's test user:
#DF <- DF[DF$datetime > 0 & DF$datetime < 1.5e+12,]
#DF <- DF[DF$site_id != 839318,]
print(nrow(DF))
DF <- filter(DF, datetime > 0.86e+12, datetime < 1.5e+12, site_id != 839318)
print(nrow(DF))

# remove sites without a known health condition
hc <- read_tsv("/home/srivbane/shared/caringbridge/data/clean_journals/health_condition.txt", col_names=c("site_id", "condition"))
print(nrow(hc))
hc <- filter(hc, condition != "custom")
print(nrow(hc))
DF <- inner_join(DF, hc, by="site_id")
print(nrow(DF))

# remove duplicated journal entries
DF <- DF %>% group_by(site_id, user_id, journal_id, datetime) %>% filter(row_number() == 1)

# determine which authors have many N or more journal posts
cutoff <- 3
sites <- DF %>% group_by(site_id) %>% summarise(n_posts = n(), min_date = min(datetime), max_date = max(datetime))
print(nrow(sites))
freq_sites <- filter(sites, n_posts >= cutoff)
print(nrow(freq_sites))

# split these site ids into training and test
num_test <- 15
in_train <- sample(freq_sites$site_id, size=nrow(freq_sites) - num_test)
print(length(in_train))

train_sites <- freq_sites %>% filter(site_id %in% in_train) %>% select(site_id, min_date) %>% arrange(site_id)
output <- as.data.frame(train_sites)
output$min_date <- format(output$min_date, scientific=FALSE)
write_tsv(output, "/home/srivbane/shared/caringbridge/data/dev/clean_journals/train_sites.txt", col_names=FALSE)

test_sites <- freq_sites %>% filter(!site_id %in% in_train) %>% select(site_id, min_date) %>% arrange(site_id)
output <- as.data.frame(test_sites)
output$min_date <- format(output$min_date, scientific=FALSE)
write_tsv(output, "/home/srivbane/shared/caringbridge/data/dev/clean_journals/test_sites.txt", col_names=FALSE)


# transform site dates into relative dates
train_sites$min_date = as.POSIXct(train_sites$min_date/1000, tz="GMT", origin="1970-01-01")
train <- inner_join(DF, train_sites, by="site_id")
train$datetime <- as.POSIXct(train$datetime/1000, tz="GMT", origin="1970-01-01")
train$relative_date = as.numeric(difftime(train$datetime, train$min_date, units="days"))

test_sites$min_date = as.POSIXct(test_sites$min_date/1000, tz="GMT", origin="1970-01-01")
test <- inner_join(DF, test_sites, by="site_id")
test$datetime <- as.POSIXct(test$datetime/1000, tz="GMT", origin="1970-01-01")
test$relative_date = as.numeric(difftime(test$datetime, test$min_date, units="days"))


# determine how many of each date bucket there should be
n_bins = 3
bins = quantile(train$relative_date, probs=seq(0,1,length.out=n_bins))
train_bins = cut(train$relative_date, bins, include.lowest=TRUE, right=FALSE)
train_bin_counts = rbind(data.frame(train_bins="all", Freq=nrow(train)), as.data.frame(table(train_bins)))
write_tsv(train_bin_counts, "/home/srivbane/shared/caringbridge/data/dev/clean_journals/train_bin_counts.txt", col_names=FALSE)
# use same bins for test set
test_bins = cut(test$relative_date, bins, include.lowest=TRUE, right=FALSE)
test_bin_counts = rbind(data.frame(test_bins="all", Freq=nrow(test)), as.data.frame(table(test_bins)))
write_tsv(test_bin_counts, "/home/srivbane/shared/caringbridge/data/dev/clean_journals/test_bin_counts.txt", col_names=FALSE)


print(train_bin_counts)
print(test_bin_counts)
